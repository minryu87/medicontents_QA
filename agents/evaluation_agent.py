# -*- coding: utf-8 -*-
"""
EvaluationAgent (FINAL)
- í”„ë¡¬í”„íŠ¸: test_prompt/llm_evaluation_prompt.txt, test_prompt/llm_regeneration_prompt.txt
- ë¡œê·¸: ê¸°ë³¸ test_logs/test/ (CLIë¡œ ë³€ê²½ ê°€ëŠ¥)
- ê¸°ì¤€: test_data/evaluation_criteria.json
- ì²´í¬ë¦¬ìŠ¤íŠ¸ CSV: test_data/medical_ad_checklist.csv (ë˜ëŠ” /mnt/test_data/medical_ad_checklist.csv)
- ë¦¬í¬íŠ¸ MD: test_data/medical-ad-report.md (ë˜ëŠ” /mnt/test_data/medical-ad-report.md)

ê¸°ëŠ¥ ìš”ì•½
1) title/content ê°•ì¸ ì¶”ì¶œ(ì¬ê·€)
2) ê·œì¹™ ìŠ¤ì½”ì–´ëŸ¬: medical_ad_checklist.csv â†’ ì •ê·œì‹/í‚¤ì›Œë“œ ìë™í™” â†’ rule_score(0~5)
3) LLM í‰ê°€: í‰ê°€ í”„ë¡¬í”„íŠ¸(JSON) â†’ llm_score(0~5)
4) ìŠ¤ì½”ì–´ ìœµí•©: final_score = max(rule_score, llm_score)
5) ìš°ì„ ìˆœìœ„ ê°€ì¤‘ ì´ì : medical-ad-report.md í…Œì´ë¸” ê¸°ë°˜(weighted_total 0~100)
6) ì„ê³„ ë¹„êµ: evaluation_criteria.json(ì—„ê²©/í‘œì¤€/ìœ ì—°) â†’ ìœ„ë°˜ íŒì •
7) ì¬ìƒì„± í”„ë¡¬í”„íŠ¸ ì ìš© â†’ ì¬í‰ê°€, Regen-Fit(0~100) ì‚°ì¶œ:
   - risk_reduction_rate (ìœ„ë°˜í•´ì†Œìœ¨)
   - guideline_adherence (ê¶Œê³  ë°˜ì˜ìœ¨)
   - flow_stability (íë¦„ ì•ˆì •ì„±)

CLI
- --criteria (ì—„ê²©|í‘œì¤€|ìœ ì—°), --max_loops, --auto-yes, --log-dir, --pattern, --debug
- --csv (--csv-path), --report (--report-path)

í•„ìˆ˜: .envì— GEMINI_API_KEY
"""

import os
import re
import csv
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Tuple, Union, Iterable

import google.generativeai as genai
from dotenv import load_dotenv



# ===== ê²½ë¡œ ê¸°ë³¸ =====
ROOT = Path(__file__).resolve().parents[1]
DEFAULT_LOG_DIR = ROOT / "app/test_logs" / "use"
PROMPTS_DIR = Path(os.environ.get('AGENTS_BASE_PATH', Path(__file__).parent)) / "utils" / "test_prompt"
DATA_DIR = Path(__file__).parent / "utils"

# ===== HTML íŒŒì‹± í•¨ìˆ˜ =====
def extract_title_and_content_from_html(html_file_path: str) -> tuple[str, str]:
    """HTML íŒŒì¼ì—ì„œ ì œëª©(í…ìŠ¤íŠ¸)ê³¼ ë³¸ë¬¸(HTML ì½”ë“œ)ì„ ì¶”ì¶œ"""
    try:
        with open(html_file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # BeautifulSoupì„ ì‚¬ìš©í•œ íŒŒì‹±
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ (h1, title íƒœê·¸ ë“±ì—ì„œ)
            title = ""
            if soup.find('h1'):
                title = soup.find('h1').get_text(strip=True)
            elif soup.find('title'):
                title = soup.find('title').get_text(strip=True)
            
            # ë³¸ë¬¸ ì¶”ì¶œ (HTML ì½”ë“œ ê·¸ëŒ€ë¡œ)
            content = html_content
            
            # HTML ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            
            print(f"ğŸ“ HTMLì—ì„œ ì¶”ì¶œ:")
            print(f"   ì œëª©: {title[:50]}..." if len(title) > 50 else f"   ì œëª©: {title}")
            print(f"   ë³¸ë¬¸ HTML í¬ê¸°: {len(content)}ì (HTML ì½”ë“œ í¬í•¨)")
            
            return title, content
            
        except ImportError:
            print("âš ï¸ BeautifulSoupì´ ì—†ì–´ ê°„ë‹¨í•œ ì •ê·œì‹ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.")
            # BeautifulSoupì´ ì—†ëŠ” ê²½ìš° ê°„ë‹¨í•œ ì •ê·œì‹ ì‚¬ìš©
            title_match = re.search(r'<h1[^>]*>(.*?)</h1>', html_content, re.IGNORECASE | re.DOTALL)
            title = title_match.group(1).strip() if title_match else ""
            
            # ContentëŠ” HTML ì½”ë“œ ê·¸ëŒ€ë¡œ (body íƒœê·¸ ë‚´ìš© ë˜ëŠ” ì „ì²´)
            body_match = re.search(r'<body[^>]*>(.*?)</body>', html_content, re.IGNORECASE | re.DOTALL)
            if body_match:
                content = f"<body>{body_match.group(1)}</body>"
            else:
                # bodyê°€ ì—†ìœ¼ë©´ ì „ì²´ HTML
                content = html_content
            
            return title, content
            
    except Exception as e:
        print(f"âŒ HTML íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        return "", ""

# ===== UI Checklist ë¡œê·¸ ìƒì„± í•¨ìˆ˜ =====
def generate_ui_checklist_logs(evaluation_data: Dict[str, Any], base_log_path: str):
    """evaluation ë¡œê·¸ì—ì„œ SEO/ì˜ë£Œë²• UI checklist ë¡œê·¸ ìƒì„±"""
    
    criteria = evaluation_data.get('modes', {}).get('criteria', '')
    by_item = evaluation_data.get('scores', {}).get('by_item', {})
    
    # criteriaë¡œ SEO vs ì˜ë£Œë²• êµ¬ë¶„
    is_legal = criteria in ['ì—„ê²©', 'í‘œì¤€', 'ìœ ì—°']
    is_seo = criteria in ['ìš°ìˆ˜', 'ì–‘í˜¸', 'ë³´í†µ']
    
    # UI checklist í˜•íƒœë¡œ ë³€í™˜
    checklist = []
    for item_id, item_data in by_item.items():
        checklist_item = {
            "name": item_data.get("name"),                    # í•­ëª©ëª… (ìˆìŒ)
            "threshold": item_data.get("threshold"),          # ê¸°ì¤€ì ìˆ˜ (ìˆìŒ)
            "grade": item_data.get("grade", None),           # í¬ìŠ¤íŠ¸ ê²€í†  ê²°ê³¼ (ì—†ìŒ, null)
            "final_score": item_data.get("final_score"),     # ì ìˆ˜ (ìˆìŒ)
            "pass_status": item_data.get("pass_status", None) # í†µê³¼ (ì—†ìŒ, null)
        }
        checklist.append(checklist_item)
    
    # íŒŒì¼ëª… ìƒì„±
    if is_seo:
        ui_log_path = base_log_path.replace('_evaluation.json', '_seo_ui_checklist.json')
        log_type = "SEO"
    elif is_legal:
        ui_log_path = base_log_path.replace('_evaluation.json', '_legal_ui_checklist.json')
        log_type = "ì˜ë£Œë²•"
    else:
        print(f"ì•Œ ìˆ˜ ì—†ëŠ” criteria: {criteria}")
        return None
    
    # UI checklist ë¡œê·¸ ì €ì¥
    try:
        with open(ui_log_path, 'w', encoding='utf-8') as f:
            json.dump(checklist, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {log_type} UI checklist ë¡œê·¸ ì €ì¥: {Path(ui_log_path).name}")
        print(f"ğŸ“Š {len(checklist)}ê°œ í•­ëª©, criteria: {criteria}")
        
        return ui_log_path
        
    except Exception as e:
        print(f"âŒ UI checklist ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

# ===== DB ì—…ë°ì´íŠ¸ í•¨ìˆ˜ =====
def auto_update_medicontent_posts(evaluation_data: Dict[str, Any], evaluation_file_path: str) -> bool:
    """evaluation ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ Medicontent Posts í…Œì´ë¸” ì—…ë°ì´íŠ¸"""
    try:
        print("ğŸ”„ Evaluation ì™„ë£Œ - ìë™ DB ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        # evaluation ë°ì´í„°ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ (criteria, scoreë§Œ)
        criteria = evaluation_data.get("modes", {}).get("criteria", "")
        weighted_total = evaluation_data.get("scores", {}).get("weighted_total", 0)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ (íŒŒì¼ëª…ì´ë‚˜ ê²½ë¡œì—ì„œ)
        timestamp = None
        source_log = evaluation_data.get("input", {}).get("source_log", "")
        
        # source_logì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ ì‹œë„
        if source_log:
            import re
            timestamp_match = re.search(r'(\d{8}_\d{6})', source_log)
            if timestamp_match:
                timestamp = timestamp_match.group(1)
        
        # íŒŒì¼ëª…ì—ì„œë„ ì¶”ì¶œ ì‹œë„
        if not timestamp:
            eval_filename = Path(evaluation_file_path).stem
            timestamp_match = re.search(r'(\d{8}_\d{6})', eval_filename)
            if timestamp_match:
                timestamp = timestamp_match.group(1)
        
        if not timestamp:
            print("âš ï¸ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ DB ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ” ì¶”ì¶œëœ íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")
        
        # criteriaì— ë”°ë¼ SEO Score vs Legal Score êµ¬ë¶„
        is_legal_score = criteria in ["ì—„ê²©", "í‘œì¤€", "ìœ ì—°"]
        is_seo_score = criteria in ["ìš°ìˆ˜", "ì–‘í˜¸", "ë³´í†µ"]
        
        # ê°™ì€ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ìƒì„±ëœ content.json ì°¾ê¸°
        eval_dir = Path(evaluation_file_path).parent
        content_pattern = f"{timestamp}_content.json"
        content_files = list(eval_dir.glob(f"**/{content_pattern}"))
        
        if not content_files:
            # ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œë„ ê²€ìƒ‰
            parent_dirs = [eval_dir.parent, eval_dir.parent.parent]
            for parent_dir in parent_dirs:
                content_files = list(parent_dir.glob(f"**/{content_pattern}"))
                if content_files:
                    break
        
        if not content_files:
            print(f"âš ï¸ íƒ€ì„ìŠ¤íƒ¬í”„ {timestamp}ì— í•´ë‹¹í•˜ëŠ” content.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        content_file = content_files[0]
        print(f"âœ… Content íŒŒì¼ ë°œê²¬: {content_file}")
        
        # ê°™ì€ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ìƒì„±ëœ HTML íŒŒì¼ ì°¾ê¸°
        html_files = []
        html_patterns = [
            f"{timestamp}.html",
            f"{timestamp}_content.html", 
            f"{timestamp}_result.html"
        ]
        
        for pattern in html_patterns:
            html_files.extend(list(eval_dir.glob(f"**/{pattern}")))
            if not html_files:
                # ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œë„ ê²€ìƒ‰
                parent_dirs = [eval_dir.parent, eval_dir.parent.parent]
                for parent_dir in parent_dirs:
                    html_files.extend(list(parent_dir.glob(f"**/{pattern}")))
                    if html_files:
                        break
            if html_files:
                break
        
        html_file = html_files[0] if html_files else None
        if html_file:
            print(f"âœ… HTML íŒŒì¼ ë°œê²¬: {html_file}")
        else:
            print(f"âš ï¸ íƒ€ì„ìŠ¤íƒ¬í”„ {timestamp}ì— í•´ë‹¹í•˜ëŠ” HTML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"   ê²€ìƒ‰í•œ íŒ¨í„´: {html_patterns}")
        
        # content.jsonì—ì„œ input_source ì¶”ì¶œ
        try:
            with open(content_file, 'r', encoding='utf-8') as f:
                content_data = json.load(f)
            
            input_source = content_data.get("meta", {}).get("input_source", "")
            
            if not input_source:
                print(f"âš ï¸ content.jsonì—ì„œ input_sourceë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            print(f"ğŸ” Contentì—ì„œ input_source ì¶”ì¶œ: {input_source}")
            
            # input_source ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if not Path(input_source).is_absolute():
                # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ROOT ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
                input_log_file = ROOT / input_source
            else:
                input_log_file = Path(input_source)
            
            print(f"ğŸ” Input ë¡œê·¸ íŒŒì¼ ê²½ë¡œ: {input_log_file}")
            
        except Exception as e:
            print(f"âŒ content.json ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            return False
        
        # input_logs.jsonì—ì„œ ì›ë˜ íƒ€ì„ìŠ¤íƒ¬í”„2 ì¶”ì¶œ
        try:
            with open(input_log_file, 'r', encoding='utf-8') as f:
                input_logs = json.load(f)
            print(f"âœ… input_logs.json ë¡œë“œ ì™„ë£Œ: {input_log_file}")
            
            # input_logs.jsonì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„2 ì¶”ì¶œ (ì›ë˜ ì‹œì‘ ì‹œì ì˜ íƒ€ì„ìŠ¤íƒ¬í”„)
            original_timestamp = None
            
            # input_logsê°€ ë°°ì—´ì¸ ê²½ìš°
            if isinstance(input_logs, list) and input_logs:
                for log_entry in input_logs:
                    if isinstance(log_entry, dict):
                        # created_atì„ ìµœìš°ì„ ìœ¼ë¡œ ì°¾ê¸°
                        for key in ['created_at', 'timestamp', 'updated_at', 'time']:
                            if key in log_entry:
                                original_timestamp = str(log_entry[key])
                                timestamp_type = key  # ì–´ë–¤ í•„ë“œì—ì„œ ê°€ì ¸ì™”ëŠ”ì§€ ê¸°ë¡
                                print(f"ğŸ” Input ë¡œê·¸ì—ì„œ '{key}' í•„ë“œ ì‚¬ìš©: {original_timestamp}")
                                break
                        if original_timestamp:
                            break
            
            # input_logsê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
            elif isinstance(input_logs, dict):
                for key in ['created_at', 'timestamp', 'updated_at', 'time']:
                    if key in input_logs:
                        original_timestamp = str(input_logs[key])
                        timestamp_type = key  # ì–´ë–¤ í•„ë“œì—ì„œ ê°€ì ¸ì™”ëŠ”ì§€ ê¸°ë¡
                        print(f"ğŸ” Input ë¡œê·¸ì—ì„œ '{key}' í•„ë“œ ì‚¬ìš©: {original_timestamp}")
                        break
            
            if not original_timestamp:
                print(f"âš ï¸ input_logs.jsonì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"   íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {str(input_logs)[:300]}...")
                return False
            
            print(f"ğŸ” Input ë¡œê·¸ì—ì„œ ì›ë˜ íƒ€ì„ìŠ¤íƒ¬í”„2 ì¶”ì¶œ: {original_timestamp}")
            
        except Exception as e:
            print(f"âŒ input_logs.json ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            return False
        
        # Medicontent Posts í…Œì´ë¸”ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„2ì™€ Updated At ë§¤ì¹­
        load_dotenv()
        
        try:
            from pyairtable import Api
            
            api = Api(os.getenv('AIRTABLE_API_KEY'))
            table = api.table(os.getenv('AIRTABLE_BASE_ID'), 'Medicontent Posts')
            
            # ëª¨ë“  ë ˆì½”ë“œë¥¼ ê°€ì ¸ì™€ì„œ ì›ë˜ íƒ€ì„ìŠ¤íƒ¬í”„2ì™€ Updated At ë§¤ì¹­
            print(f"ğŸ” Medicontent Postsì—ì„œ Updated At ì‹œê°„ì´ ì›ë˜ íƒ€ì„ìŠ¤íƒ¬í”„2 '{original_timestamp}'ì™€ ë§¤ì¹­ë˜ëŠ” ë ˆì½”ë“œ ê²€ìƒ‰...")
            all_records = table.all()
            print(f"ğŸ“Š ì´ {len(all_records)}ê°œì˜ ë ˆì½”ë“œë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            
            # ì›ë˜ íƒ€ì„ìŠ¤íƒ¬í”„2ë¥¼ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (YYYY-MM-DD HH:MM)
            iso_timestamp = None
            
            # original_timestampê°€ 20250821_165228 í˜•ì‹ì¸ ê²½ìš°
            if '_' in original_timestamp and len(original_timestamp) == 15:
                date_part = original_timestamp[:8]  # 20250821
                time_part = original_timestamp[9:]  # 165228
                hour = time_part[:2]       # 16
                minute = time_part[2:4]    # 52
                
                # ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜: YYYY-MM-DD HH:MM (ì´ˆ ì œì™¸)
                iso_timestamp = f"{original_timestamp[:4]}-{original_timestamp[4:6]}-{original_timestamp[6:8]} {hour}:{minute}"
                
            else:
                # ë‹¤ë¥¸ í˜•ì‹ì˜ ê²½ìš° YYYY-MM-DD HH:MM í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì´ˆ ì œê±°)
                import re
                # YYYY-MM-DD HH:MM:SS í˜•ì‹ì—ì„œ ì´ˆ ì œê±°
                if re.match(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', original_timestamp):
                    iso_timestamp = original_timestamp[:16]  # YYYY-MM-DD HH:MMê¹Œì§€ë§Œ
                else:
                    iso_timestamp = original_timestamp
                
            print(f"ğŸ”„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜: {original_timestamp} â†’ {iso_timestamp}")
            
            matched_record = None
            original_post_id = None
            
            # 1ì°¨: Created At ìš°ì„  ë§¤ì¹­ (ìƒì„± ì‹œê°„ì€ ë³€ê²½ë˜ì§€ ì•ŠìŒ)
            print(f"ğŸ”„ 1ì°¨ ì‹œë„: Created At ë§¤ì¹­...")
            for i, record in enumerate(all_records):
                created_at = record['fields'].get('Created At', '')
                record_post_id = record['fields'].get('Post Id', '')
                
                # ë””ë²„ê¹…: ì²˜ìŒ ëª‡ ê°œ ë ˆì½”ë“œì˜ ìƒì„¸ ì •ë³´ ì¶œë ¥
                if i < 3:
                    print(f"ğŸ” ë ˆì½”ë“œ {i+1}: PostID='{record_post_id}', Created At='{created_at}'")
                
                if created_at and iso_timestamp:
                    try:
                        from datetime import datetime, timezone, timedelta
                        # Created Atì„ í•œêµ­ ì‹œê°„ëŒ€ë¡œ ë³€í™˜
                        dt_utc = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                        korea_tz = timezone(timedelta(hours=9))
                        dt_korea = dt_utc.astimezone(korea_tz)
                        airtable_formatted = dt_korea.strftime('%Y-%m-%d %H:%M')
                        
                        if i < 3:
                            print(f"   ğŸ“… Created At ë³€í™˜: '{created_at}' â†’ '{airtable_formatted}' (ì°¾ëŠ”ê°’: '{iso_timestamp}')")
                        
                        if iso_timestamp == airtable_formatted:
                            matched_record = record
                            original_post_id = record_post_id
                            print(f"âœ… Created At ë§¤ì¹­ ì„±ê³µ! (ìƒì„± ì‹œê°„ ê¸°ì¤€)")
                            print(f"   evaluation íƒ€ì„ìŠ¤íƒ¬í”„1: {timestamp}")
                            print(f"   input íƒ€ì„ìŠ¤íƒ¬í”„2: {original_timestamp}")
                            print(f"   ISO ë³€í™˜: {iso_timestamp}")
                            print(f"   Airtable Created At: {created_at}")
                            print(f"   ë³€í™˜ëœ ì‹œê°„: {airtable_formatted}")
                            print(f"   ì°¾ì€ PostID: {original_post_id}")
                            break
                    except Exception as e:
                        if i < 3:
                            print(f"   âŒ Created At íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                        continue
            
            # 2ì°¨: Created At ì‹¤íŒ¨ì‹œ Updated Atìœ¼ë¡œ fallback
            if not matched_record:
                print(f"ğŸ”„ 2ì°¨ ì‹œë„: Created At ë§¤ì¹­ ì‹¤íŒ¨ â†’ Updated At fallback...")
                
                matched_count = 0
                for i, record in enumerate(all_records):
                    updated_at = record['fields'].get('Updated At', '')
                    record_post_id = record['fields'].get('Post Id', '')
                    
                    if i < 3:
                        print(f"ğŸ” Fallback ë ˆì½”ë“œ {i+1}: PostID='{record_post_id}', Updated At='{updated_at}'")
                    
                    if updated_at and iso_timestamp:
                        try:
                            from datetime import datetime, timezone, timedelta
                            dt_utc = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))
                            korea_tz = timezone(timedelta(hours=9))
                            dt_korea = dt_utc.astimezone(korea_tz)
                            airtable_formatted = dt_korea.strftime('%Y-%m-%d %H:%M')
                            
                            if i < 3:
                                print(f"   ğŸ“… Updated At ë³€í™˜: '{updated_at}' â†’ '{airtable_formatted}' (ì°¾ëŠ”ê°’: '{iso_timestamp}')")
                            
                            if iso_timestamp == airtable_formatted:
                                matched_record = record
                                original_post_id = record_post_id
                                print(f"âœ… Updated At ë§¤ì¹­ ì„±ê³µ! (fallback)")
                                print(f"   ì°¾ì€ PostID: {original_post_id}")
                                break
                            else:
                                matched_count += 1
                        except Exception as e:
                            if i < 3:
                                print(f"   âŒ Updated At íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                            continue
                
                print(f"ğŸ”¢ Updated Atìœ¼ë¡œ {matched_count}ê°œ ë ˆì½”ë“œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
            
            if not matched_record:
                print(f"âŒ Created Atê³¼ Updated At ëª¨ë‘ì—ì„œ ë§¤ì¹­ ì‹¤íŒ¨")
                print(f"   ì›ë³¸ íƒ€ì„ìŠ¤íƒ¬í”„2: {original_timestamp}")
                print(f"   ë³€í™˜ëœ ISO í˜•ì‹: {iso_timestamp}")
                print("ğŸ“‹ ì „ì²´ Medicontent Posts ë ˆì½”ë“œ ëª©ë¡ (Created At ê¸°ì¤€):")
                for i, record in enumerate(all_records):  # ì „ì²´ ë ˆì½”ë“œ
                    post_id = record['fields'].get('Post Id', '')
                    created_at = record['fields'].get('Created At', '')
                    updated_at = record['fields'].get('Updated At', '')
                    try:
                        from datetime import datetime, timezone, timedelta
                        # Created Atì„ í•œêµ­ ì‹œê°„ëŒ€ë¡œ ë³€í™˜
                        if created_at:
                            dt_utc = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                            korea_tz = timezone(timedelta(hours=9))
                            dt_korea = dt_utc.astimezone(korea_tz)
                            created_formatted = dt_korea.strftime('%Y-%m-%d %H:%M')
                            match_status = "âœ… ë§¤ì¹­!" if created_formatted == iso_timestamp else ""
                            print(f"   {i+1}. PostID: '{post_id}', Created At: '{created_at}' â†’ '{created_formatted}' {match_status}")
                        else:
                            print(f"   {i+1}. PostID: '{post_id}', Created At: ì—†ìŒ")
                    except:
                        print(f"   {i+1}. PostID: '{post_id}', Created At: '{created_at}' (íŒŒì‹±ì‹¤íŒ¨)")
                return False
            
            record_id = matched_record['id']
            
            # HTML íŒŒì¼ì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
            title = ""
            content = ""
            if html_file:
                title, content = extract_title_and_content_from_html(str(html_file))
            else:
                print("âš ï¸ HTML íŒŒì¼ì´ ì—†ì–´ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # í˜„ì¬ ë ˆì½”ë“œì—ì„œ ê¸°ì¡´ SEO Scoreì™€ Legal Score í™•ì¸
            current_fields = matched_record['fields']
            existing_seo_score = current_fields.get('SEO Score')
            existing_legal_score = current_fields.get('Legal Score')
            
            # HTML ID ìƒì„± (íŒŒì¼ëª…ì—ì„œ .html í™•ì¥ì ì œê±°)
            html_id = f"{timestamp}_content"
            
            # ì—…ë°ì´íŠ¸í•  ë°ì´í„° ì¤€ë¹„
            update_data = {
                'Updated At': datetime.now().strftime('%Y-%m-%d %H:%M'),
                'HTML ID': html_id
            }
            
            # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ê°€
            if title:
                update_data['Title'] = title
                print(f"ğŸ“ ì œëª© ì¶”ê°€: {title[:50]}...")
            
            if content:
                update_data['Content'] = content
                print(f"ğŸ“ HTML ë³¸ë¬¸ ì¶”ê°€: {len(content)}ì (ì „ì²´ HTML íŒŒì¼)")
            
            # SEO Score ë˜ëŠ” Legal Score ì¶”ê°€
            if is_seo_score:
                update_data['SEO Score'] = weighted_total
                print(f"ğŸ“ˆ SEO Score ì„¤ì •: {weighted_total} (criteria: {criteria})")
            elif is_legal_score:
                update_data['Legal Score'] = weighted_total
                print(f"âš–ï¸ Legal Score ì„¤ì •: {weighted_total} (criteria: {criteria})")
            
            # ë‘˜ ë‹¤ ìˆì„ ë•Œë§Œ ì‘ì—… ì™„ë£Œë¡œ ë³€ê²½
            will_have_seo = existing_seo_score or is_seo_score
            will_have_legal = existing_legal_score or is_legal_score
            
            if will_have_seo and will_have_legal:
                update_data['Status'] = 'ì‘ì—… ì™„ë£Œ'
                print(f"âœ… SEO Scoreì™€ Legal Score ëª¨ë‘ ìˆìŒ â†’ Status: ì‘ì—… ì™„ë£Œ")
            else:
                print(f"â³ ì•„ì§ í•œìª½ Scoreë§Œ ìˆìŒ â†’ Status ìœ ì§€")
                print(f"   SEO Score: {'âœ…' if will_have_seo else 'âŒ'}")
                print(f"   Legal Score: {'âœ…' if will_have_legal else 'âŒ'}")
            
            # Airtable ì—…ë°ì´íŠ¸ ì‹¤í–‰
            table.update(record_id, update_data)
            
            print(f"âœ… Medicontent Posts ìë™ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            print(f"   íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")
            print(f"   PostID: {original_post_id}")
            print(f"   Record ID: {record_id}")
            print(f"   Status: ì‘ì—… ì™„ë£Œ")
            print(f"   Title: {title[:50]}..." if title else "")
            print(f"   Content length: {len(content)}")
            print(f"   Score: {weighted_total} ({criteria})")
            
            return True
            
        except ImportError:
            print("âš ï¸ pyairtable ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ DB ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ Airtable ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
            
    except Exception as e:
        print(f"âŒ ìë™ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

EVAL_PROMPT_PATH = PROMPTS_DIR / "llm_evaluation_prompt.txt"
REGEN_PROMPT_PATH = PROMPTS_DIR / "llm_regeneration_prompt.txt"
SEO_PROMPT_PATH = PROMPTS_DIR / "seo_evaluation_prompt.txt"
CRITERIA_PATH = DATA_DIR / "evaluation_criteria.json"
SEO_CRITERIA_PATH = DATA_DIR / "seo_evaluation_criteria.json"
DEFAULT_CSV_PATHS = [DATA_DIR / "medical_ad_checklist.csv", Path("/mnt/test_data/medical_ad_checklist.csv")]
DEFAULT_REPORT_PATHS = [DATA_DIR / "medical-ad-report.md", Path("/mnt/test_data/medical-ad-report.md")]

# ===== ì²´í¬ë¦¬ìŠ¤íŠ¸ ëª…ì¹­ =====
CHECKLIST_NAMES = {
    1: "í—ˆìœ„Â·ê³¼ì¥ í‘œí˜„", 2: "ì¹˜ë£Œê²½í—˜ë‹´", 3: "ë¹„ê¸‰ì—¬ ì§„ë£Œë¹„ í• ì¸", 4: "ì‚¬ì „ì‹¬ì˜ ë¯¸ì´í–‰",
    5: "ì¹˜ë£Œ ì „í›„ ì‚¬ì§„", 6: "ì „ë¬¸ì˜ í—ˆìœ„ í‘œì‹œ", 7: "í™˜ì ìœ ì¸Â·ì•Œì„ ", 8: "ë¹„ì˜ë£Œì¸ ì˜ë£Œê´‘ê³ ",
    9: "ê°ê´€ì  ê·¼ê±° ë¶€ì¡±", 10: "ë¹„êµ ê´‘ê³ ", 11: "ê¸°ì‚¬í˜• ê´‘ê³ ", 12: "ë¶€ì‘ìš© ì •ë³´ ëˆ„ë½",
    13: "ì¸ì¦Â·ë³´ì¦ í—ˆìœ„í‘œì‹œ", 14: "ê°€ê²© ì •ë³´ ì˜¤í‘œì‹œ", 15: "ì—°ë½ì²˜ ì •ë³´ ì˜¤ë¥˜",
}

SEO_CHECKLIST_NAMES = {
    1: "ì œëª© ê¸€ììˆ˜ (ê³µë°± í¬í•¨)", 2: "ì œëª© ê¸€ììˆ˜ (ê³µë°± ì œì™¸)", 3: "ë³¸ë¬¸ ê¸€ììˆ˜ (ê³µë°± í¬í•¨)",
    4: "ë³¸ë¬¸ ê¸€ììˆ˜ (ê³µë°± ì œì™¸)", 5: "ì´ í˜•íƒœì†Œ ê°œìˆ˜", 6: "ì´ ìŒì ˆ ê°œìˆ˜",
    7: "ì´ ë‹¨ì–´ ê°œìˆ˜", 8: "ì–´ë·°ì§• ë‹¨ì–´ ê°œìˆ˜", 9: "ë³¸ë¬¸ ì´ë¯¸ì§€"
}

# ===== ë¦¬í¬íŠ¸ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’) =====
DEFAULT_REPORT_WEIGHTS = {
    "1": 8.6, "2": 8.0, "3": 8.0, "4": 8.0, "5": 7.0,
    "6": 7.0, "7": 8.0, "8": 7.4, "9": 6.4, "10": 6.4,
    "11": 6.0, "12": 6.0, "13": 6.0, "14": 6.0, "15": 5.5
}

# ===== ê·œì¹™ ì—”ì§„ ê¸°ë³¸ íŒ¨í„´(ë¶€ì¡±ë¶„ì€ CSVì—ì„œ ë³´ê°•) =====
BASE_PATTERNS = {
    1: [r"\b100\s*%\b", r"ë¶€ì‘ìš©\s*ì—†(ìŒ|ë‹¤)", r"\bìµœê³ \b", r"\bìœ ì¼(í•œ)?\b", r"ì™„ì „\s*ë¬´í†µ"],
    2: [r"í›„ê¸°|ê²½í—˜ë‹´|ë¦¬ë·°", r"ë§Œì¡±ë„", r"ì¹˜ë£Œ\s*ê³¼ì •", r"ì¹˜ë£Œ\s*ê²°ê³¼", r"í˜‘ì°¬|ì œê³µ\s*ë°›"],
    3: [r"\d{1,3}\s?%(\s*í• ì¸)?", r"ì´ë²¤íŠ¸\s*ê°€", r"í–‰ì‚¬\s*ê°€", r"\bì›\s*ë¶€í„°\b"],
    4: [r"ì‹¬ì˜ë²ˆí˜¸", r"ì‹¬ì˜\s*ë¯¸ì´í–‰|ë¯¸ì‹¬ì˜"],
    5: [r"\bì „í›„\b", r"\bbefore\b", r"\bafter\b", r"!\[.*\]\(.*\)", r"<img[^>]+>"],
    6: [r"ì „ë¬¸ì˜", r"ì „ë¬¸ë³‘ì›", r"ì„í”Œë€íŠ¸\s*ì „ë¬¸ì˜", r"êµì •\s*ì „ë¬¸ë³‘ì›"],
    7: [r"ë¦¬ë·°\s*ì´ë²¤íŠ¸", r"ì¶”ì²¨", r"ì‚¬ì€í’ˆ", r"ë¦¬ë·°\s*ì‘ì„±\s*ì‹œ", r"ëŒ€ê°€|í¬ì¸íŠ¸|ê¸°í”„í‹°ì½˜"],
    8: [r"ì¸í”Œë£¨ì–¸ì„œ|ì¼ë°˜ì¸\s*ê´‘ê³ ", r"ì œíœ´\s*í¬ìŠ¤íŒ…"],
    9: [r"ì„ìƒê²°ê³¼|ì—°êµ¬ê²°ê³¼|ë°ì´í„°", r"ê·¼ê±°\s*ì—†(ìŒ|ë‹¤)"],
    10:[r"íƒ€\s*ë³‘ì›|ë‹¤ë¥¸\s*ë³‘ì›", r"ìµœì´ˆ|ìµœê³ |ìœ ì¼\s*ë¹„êµ", r"ë³´ë‹¤\s*ë‚«"],
    11:[r"ê¸°ì‚¬í˜•|ë³´ë„ìë£Œ|ì¸í„°ë·°\s*í˜•íƒœ", r"ì „ë¬¸ê°€\s*ì˜ê²¬\s*í˜•ì‹"],
    12:[r"ë¶€ì‘ìš©|ì£¼ì˜ì‚¬í•­|ê°œì¸ì°¨", r"ë¦¬ìŠ¤í¬|í•©ë³‘ì¦"],
    13:[r"ì¸ì¦|ìƒì¥|ê°ì‚¬ì¥|ì¶”ì²œ", r"ê³µì‹\s*ì¸ì¦"],
    14:[r"ì›\s*ë¶€í„°|ìµœì €ê°€|í• ì¸\s*ê°€", r"ì¶”ê°€\s*ë¹„ìš©|ë¶€ê°€ì„¸"],
    15:[r"ë³‘ì›ëª…|ì£¼ì†Œ|ì „í™”|ì—°ë½ì²˜", r"ì˜¤ë¥˜|ë¶ˆì¼ì¹˜"],
}

# ===== SEO ë©”íŠ¸ë¦­ ê³„ì‚° (ì •ì œ ìœ í‹¸ ì¶”ê°€) =====
# --- SEO ì¸¡ì • ì „ìš©: ì´ë¯¸ì§€ ê°ì§€+ì •ì œ ---
_IMG_EXT_RE = r'(?:jpg|jpeg|png|gif)'
_MKDOWN_IMG_RE = re.compile(r'!\[[^\]]*\]\(([^)]+)\)', re.IGNORECASE)   # ![alt](url)
_HTML_IMG_RE   = re.compile(r'<img\b[^>]*>', re.IGNORECASE)             # <img ...>
_PAREN_IMG_RE  = re.compile(r'\(([^()\s]+?\.' + _IMG_EXT_RE + r')\)', re.IGNORECASE)  # (file.ext)

def _extract_images_and_clean_text(raw: str) -> Tuple[str, int]:
    """
    - ì´ë¯¸ì§€ ê°œìˆ˜: ë§ˆí¬ë‹¤ìš´/HTML/ê´„í˜¸í˜• íŒŒì¼ëª… 3ì¢…ì„ í•©ì‚° (ì¤‘ë³µ ë°©ì§€ ìœ„í•´ ìˆœì°¨ ì œê±°)
    - ì •ì œ í…ìŠ¤íŠ¸: ì´ë¯¸ì§€ í‘œí˜„(ë§ˆí¬ë‹¤ìš´/HTML/ê´„í˜¸í˜• íŒŒì¼ëª…) ëª¨ë‘ ì œê±°,
                  ì¤„ë°”ê¿ˆ/íƒ­â†’ê³µë°±, ê³µë°± ë‹¤ì¤‘ â†’ 1ì¹¸ìœ¼ë¡œ ì¶•ì•½
    """
    if not isinstance(raw, str):
        return "", 0

    text = raw

    # 1) ë§ˆí¬ë‹¤ìš´ ì´ë¯¸ì§€: ì¹´ìš´íŠ¸ & ì œê±°
    md_hits = _MKDOWN_IMG_RE.findall(text)
    text = _MKDOWN_IMG_RE.sub(' ', text)

    # 2) HTML ì´ë¯¸ì§€: ì¹´ìš´íŠ¸ & ì œê±°
    html_hits = _HTML_IMG_RE.findall(text)
    text = _HTML_IMG_RE.sub(' ', text)

    # 3) ê´„í˜¸í˜• íŒŒì¼ëª…: ì¹´ìš´íŠ¸ & ì œê±°  e.g., (ab.png)
    paren_hits = _PAREN_IMG_RE.findall(text)
    text = _PAREN_IMG_RE.sub(' ', text)

    # 4) ì¤„ë°”ê¿ˆ/íƒ­ ì œê±°(â†’ ê³µë°± 1ì¹¸), ê³µë°± ë‹¤ì¤‘ ì¶•ì•½
    text = text.replace('\r', ' ').replace('\n', ' ').replace('\t', ' ')
    text = re.sub(r'\s+', ' ', text).strip()

    image_count = len(md_hits) + len(html_hits) + len(paren_hits)
    return text, image_count

def _calculate_morphemes(text: str) -> int:
    """í˜•íƒœì†Œ ê°œìˆ˜ ê³„ì‚° (kiwipiepy ì‚¬ìš©)"""
    from kiwipiepy import Kiwi
    kiwi = Kiwi()
    result = kiwi.analyze(text)
    tokens, score = result[0]
    morphemes = [token.form for token in tokens]
    return len(morphemes)

def _count_syllables_extended(text: str) -> int:
    """ìŒì ˆ ê°œìˆ˜ ê³„ì‚° (í•œê¸€ + ì˜ë¬¸)"""
    import unicodedata
    text = unicodedata.normalize('NFC', text)
    syllables = 0
    for ch in text:
        if 0xAC00 <= ord(ch) <= 0xD7A3:  # í•œê¸€ ì™„ì„±í˜•
            syllables += 1
        elif ch.isascii() and ch.isalpha():  # ì˜ë¬¸ë§Œ
            syllables += 1
    return syllables

def calculate_seo_metrics(title: str, content: str) -> Dict[str, int]:
    """SEO í‰ê°€ìš© ì‹¤ì œ ì¸¡ì •ê°’ (ë Œë” ê²°ê³¼ ê¸°ì¤€: ì´ë¯¸ì§€/alt/íŒŒì¼ëª… ì œê±°, ì¤„ë°”ê¿ˆ ì œì™¸)"""
    import re

    # --- ì œëª©(ê·¸ëŒ€ë¡œ) ---
    title_with_space = len(title)
    title_without_space = len(title.replace(" ", ""))
    print(f"DEBUG - ì œëª©: '{title}'")
    print(f"DEBUG - ê³µë°± í¬í•¨: {title_with_space}, ê³µë°± ì œì™¸: {title_without_space}")

    # --- ë³¸ë¬¸: ì •ì œ + ì´ë¯¸ì§€ ì¹´ìš´íŠ¸ ---
    cleaned, image_count = _extract_images_and_clean_text(content)

    # 3/4. ë³¸ë¬¸ ê¸€ììˆ˜
    content_with_space = len(cleaned)
    content_without_space = len(re.sub(r'\s+', '', cleaned))  # ëª¨ë“  ê³µë°± ì œê±°(ê°œí–‰ í¬í•¨)

    # 5. í˜•íƒœì†Œ(ì •ì œ í…ìŠ¤íŠ¸ ê¸°ì¤€)
    morpheme_count = _calculate_morphemes(cleaned)

    # 6. ìŒì ˆ(ì •ì œ í…ìŠ¤íŠ¸ ê¸°ì¤€)
    syllable_count = _count_syllables_extended(cleaned)

    # 7. ë‹¨ì–´(ì •ì œ í…ìŠ¤íŠ¸ ê¸°ì¤€)
    word_count = len(re.findall(r'[\wê°€-í£]+', cleaned))

    # 8. ì–´ë·°ì§• ë‹¨ì–´(ì •ì œ í…ìŠ¤íŠ¸ ê¸°ì¤€)
    abusing_patterns = [
        r'19ê¸ˆ', r'ì„±ì¸', r'ìœ í•´', r'ë„ë°•', r'ë¶ˆë²•', r'ì‚¬ê¸°',
        r'100%', r'ì™„ì „ë¬´ë£Œ', r'ëŒ€ë°•', r'ì§±', r'í—', r'1ë“±', r'ìµœê³ ', r'ìµœê°•', r'ì™„ë²½', r'ë³´ì¥', r'ì™„ì¹˜', r'ì¹˜ë£Œë³´ì¥',
        r'ì¦‰ì‹œ', r'ë‹¹ì¼', r'ë°”ë¡œ', r'ì§€ê¸ˆ\s*ë‹¹ì¥', r'ë°˜ë“œì‹œ', r'ì ˆëŒ€', r'ë¬´ì¡°ê±´',
        r'ì „ë¶€', r'ì „ì„¸ê³„', r'êµ­ë‚´ìœ ì¼', r'ë…ì ', r'ìœ ì¼ë¬´ì´', r'ë² ìŠ¤íŠ¸', r'í”„ë¦¬ë¯¸ì—„',
        r'ëª…í’ˆ', r'ì´ˆíŠ¹ê°€', r'íŒŒê²©', r'ë¬´ë£Œ', r'ê³µì§œ', r'í• ì¸', r'ì´ë²¤íŠ¸', r'ì‚¬ì€í’ˆ',
        r'í•œì •', r'ë§ˆê°ì„ë°•', r'ì¬ê³ ì†Œì§„', r'ì„ ì°©ìˆœ', r'ë‹¨ë…', r'ìµœì´ˆ', r'ìœ ì¼',
        r'ì™„ì „', r'í•„ìˆ˜', r'ê°•ë ¥ì¶”ì²œ'
    ]
    abusing_count = sum(len(re.findall(pat, cleaned, re.IGNORECASE)) for pat in abusing_patterns)

    return {
        1: title_with_space,
        2: title_without_space,
        3: content_with_space,
        4: content_without_space,
        5: morpheme_count,
        6: syllable_count,
        7: word_count,
        8: abusing_count,
        9: image_count
    }

# ===== ìœ í‹¸ =====
def _nowstamp() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def _read_text(p: Path) -> str:
    if not p.exists():
        raise FileNotFoundError(f"íŒŒì¼ ì—†ìŒ: {p}")
    return p.read_text(encoding="utf-8")

def _read_json(p: Path) -> Any:
    if not p.exists():
        raise FileNotFoundError(f"íŒŒì¼ ì—†ìŒ: {p}")
    return json.loads(p.read_text(encoding="utf-8"))

def _write_json(p: Path, obj: Any):
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")

def _latest(log_dir: Path, glob_pat: Union[str, List[str]]) -> Path:
    # log_dir ì•ˆì˜ ìµœì‹  ë‚ ì§œ í´ë” ì„ íƒ
    date_dirs = [p for p in log_dir.iterdir() if p.is_dir()]
    if not date_dirs:
        raise FileNotFoundError(f"ë‚ ì§œ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_dir}")

    latest_date_dir = max(date_dirs, key=lambda d: d.stat().st_mtime)

    # ê·¸ ì•ˆì—ì„œ glob íƒìƒ‰
    patterns = glob_pat if isinstance(glob_pat, list) else [glob_pat]
    candidates: List[Path] = []
    for pat in patterns:
        candidates.extend(list(latest_date_dir.glob(pat)))

    if not candidates:
        listing = "\n".join(sorted([p.name for p in latest_date_dir.glob('*')]))
        raise FileNotFoundError(
            f"ìµœì‹  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {latest_date_dir}/{patterns}\n"
            f"í˜„ì¬ íŒŒì¼ ëª©ë¡:\n{listing if listing else '(ë¹„ì–´ ìˆìŒ)'}"
        )

    candidates.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    return candidates[0]

# ===== LLM =====
def _setup_llm():
    load_dotenv()
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("GEMINI_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-1.5-pro")

def _extract_json(raw: str) -> Dict[str, Any]:
    if not raw:
        raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    text = raw.strip()
    if text.startswith("```"):
        text = re.sub(r"^```[a-zA-Z0-9]*\s*", "", text)
        text = re.sub(r"\s*```$", "", text)
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        text = text[start:end+1]
    text = text.strip().strip("`").strip()
    return json.loads(text)

def _call_llm(model, prompt: str) -> Dict[str, Any]:
    resp = model.generate_content(prompt)
    text = getattr(resp, "text", "") or ""
    if not text:
        try:
            cand0 = resp.candidates[0]
            parts = getattr(getattr(cand0, "content", None), "parts", []) or []
            text = "".join(getattr(p, "text", "") for p in parts if getattr(p, "text", ""))
        except Exception:
            pass
    if not text:
        raise RuntimeError("LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨(ë¹ˆ ì‘ë‹µ). í”„ë¡¬í”„íŠ¸ ë˜ëŠ” ì•ˆì „í•„í„° í™•ì¸.")
    return _extract_json(text)

# ===== ì¬ê·€ íƒìƒ‰ ë„êµ¬ =====
def _iter_paths(obj: Any, prefix: Tuple=()) -> Iterable[Tuple[Tuple, Any]]:
    if isinstance(obj, dict):
        for k, v in obj.items():
            yield from _iter_paths(v, prefix + (k,))
    elif isinstance(obj, list):
        for i, v in enumerate(obj):
            yield from _iter_paths(v, prefix + (i,))
    else:
        yield (prefix, obj)

def _path_to_str(path: Tuple) -> str:
    return ".".join(str(p) for p in path)

TITLE_KEY_HINTS = ["title","post_title","page_title","doc_title","headline","h1"]
CONTENT_KEY_HINTS = ["content","body","post_content","article","markdown","md","html","text",
                     "paragraph","paragraphs","section","sections","blocks","document","value"]

def _score_title_candidate(s: str) -> float:
    if not isinstance(s, str): return -1
    l = len(s.strip())
    if l < 3: return -1
    score = 0.0
    if 10 <= l <= 120: score += 2.0
    elif l <= 200: score += 1.0
    if sum(ch in "#*{}[]" for ch in s) > 5: score -= 0.5
    return score + min(l/200.0, 1.0)

def _normalize_block_to_text(val: Any) -> str:
    if isinstance(val, str):
        return val
    if isinstance(val, list):
        parts = []
        for it in val:
            if isinstance(it, str):
                parts.append(it)
            elif isinstance(it, dict):
                for k in ["text","content","paragraph","markdown","md","html","value"]:
                    v = it.get(k)
                    if isinstance(v, str) and v.strip():
                        parts.append(v)
                        break
        return "\n\n".join(p for p in parts if p.strip())
    if isinstance(val, dict):
        for k in ["markdown","md","html","text","content","body","value"]:
            v = val.get(k)
            if isinstance(v, str) and v.strip():
                return v
        for k in ["paragraphs","sections","blocks"]:
            v = val.get(k)
            if isinstance(v, list) and v:
                s = _normalize_block_to_text(v)
                if s.strip(): return s
    return ""

def _extract_title_content(clog: Dict[str, Any]) -> Tuple[str, str, Dict[str, Any]]:
    cand_titles: List[Tuple[str, str, float]] = []
    cand_contents: List[Tuple[str, str, int]] = []
    for path, val in _iter_paths(clog):
        pstr = _path_to_str(path)
        key_lower = str(path[-1]).lower() if path else ""

        # ë””ë²„ê·¸ ì¶œë ¥ ì¶”ê°€
        if "title" in key_lower:
            print(f"DEBUG - ë°œê²¬ëœ title ê´€ë ¨ í‚¤: path={pstr}, key_lower='{key_lower}', val='{val}', ë§¤ì¹˜={any(key_lower == h for h in TITLE_KEY_HINTS)}")
    
        if any(key_lower == h for h in TITLE_KEY_HINTS) and isinstance(val, str):
            s = val.strip()
            if s: 
                score = _score_title_candidate(s)
                print(f"DEBUG - title í›„ë³´ ì¶”ê°€: '{s}', score={score}")
                cand_titles.append((pstr, s, score))
                
        if any(h in key_lower for h in CONTENT_KEY_HINTS):
            text = _normalize_block_to_text(val)
            if not text and isinstance(val, str):
                text = val
            if isinstance(text, str) and text.strip():
                cand_contents.append((pstr, text, len(text)))
    title, title_path = "", ""
    if cand_titles:
        cand_titles.sort(key=lambda x: x[2], reverse=True)
        title_path, title, _ = cand_titles[0]
    else:
        # ë¨¼ì € ìµœìƒìœ„ ë ˆë²¨ì˜ title í‚¤ë¥¼ ì§ì ‘ í™•ì¸ (ìš°ì„ ìˆœìœ„)
        for k in ["title", "Title", "post_title"]:
            if k in clog and isinstance(clog[k], str) and clog[k].strip():
                title = clog[k].strip()
                title_path = k
                break
        
        # ì§ì ‘ í‚¤ ì ‘ê·¼ì´ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ selected.title í™•ì¸
        if not title:
            if isinstance(clog.get("selected"), dict) and isinstance(clog["selected"].get("title"), str):
                title = clog["selected"]["title"].strip()
                title_path = "selected.title"
    content, content_path = "", ""
    if cand_contents:
        cand_contents.sort(key=lambda x: (x[2] >= 300, x[2]), reverse=True)
        content_path, content, _ = cand_contents[0]
    else:
        if isinstance(clog.get("content"), list):
            content = "\n\n".join(map(str, clog["content"])); content_path = "content(list)"
        if not content:
            for parent in ["result","data"]:
                if isinstance(clog.get(parent), dict):
                    v = clog[parent].get("content") or clog[parent].get("body") or clog[parent].get("post_content")
                    s = _normalize_block_to_text(v)
                    if s.strip():
                        content = s.strip(); content_path = f"{parent}.content/body/post_content"; break
    dbg = {
        "title_path": title_path,
        "content_path": content_path,
        "title_candidates": [{"path":p,"len":len(v),"score":sc} for (p,v,sc) in cand_titles[:5]],
        "content_candidates": [{"path":p,"len":l} for (p,_,l) in cand_contents[:5]],
    }
    return title.strip(), content.strip(), dbg

# ===== CSV ë¡œë“œ/ê·œì¹™ ì»´íŒŒì¼ =====
def _find_existing(paths: List[Path]) -> Path:
    for p in paths:
        if p.exists(): return p
    raise FileNotFoundError(f"ê²½ë¡œë“¤ ì¤‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {paths}")

def load_checklist_csv(path: Path) -> List[Dict[str, str]]:
    rows = []
    with path.open("r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for r in reader:
            # í—¤ë” ì˜ˆ: ë²ˆí˜¸,í•­ëª©ëª…,í•­ëª©ì„¤ëª…,í‰ê°€ë°©ë²•,ìœ„ë°˜ìœ„í—˜ë„
            rows.append({k.strip(): (v.strip() if isinstance(v,str) else v) for k,v in r.items()})
    return rows

def compile_patterns(rows: List[Dict[str,str]]) -> Dict[int, List[re.Pattern]]:
    patterns: Dict[int, List[re.Pattern]] = {}
    for r in rows:
        try:
            idx = int(r.get("ë²ˆí˜¸") or r.get("no") or r.get("index"))
        except Exception:
            continue
        p_list = BASE_PATTERNS.get(idx, []).copy()
        eval_method = (r.get("í‰ê°€ë°©ë²•") or "").replace("<br>", "\n")
        # í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ(ê°„ë‹¨)
        for kw in ["ìµœê³ ","ìœ ì¼","ì™„ì „","100%","ë¶€ì‘ìš© ì—†ìŒ","ì´ë²¤íŠ¸","í• ì¸","ì „í›„","before","after",
                   "ë¦¬ë·°","í›„ê¸°","í˜‘ì°¬","ê°€ê²©","ì›ë¶€í„°","ì‹¬ì˜ë²ˆí˜¸","ì „ë¬¸ì˜","ì „ë¬¸ë³‘ì›","ì£¼ì˜ì‚¬í•­","ë¶€ì‘ìš©","ê°œì¸ì°¨",
                   "ì¸ì¦","ìƒì¥","ê°ì‚¬ì¥","ì¶”ì²œ","ê¸°ì‚¬í˜•","ë³´ë„ìë£Œ","ì¸í„°ë·°","íƒ€ ë³‘ì›","ìµœì´ˆ","ìœ ì¼"]:
            if kw in eval_method and kw not in p_list:
                p_list.append(re.escape(kw))
        try:
            patterns[idx] = [re.compile(p, flags=re.I) for p in p_list]
        except re.error:
            # ì˜ëª»ëœ íŒ¨í„´ì€ ìŠ¤í‚µ
            patterns[idx] = [re.compile(re.escape(p), flags=re.I) for p in p_list if p]
    return patterns

def rule_score_item(idx: int, text: str, pats: Dict[int, List[re.Pattern]]) -> Tuple[int, List[str]]:
    if idx not in pats or not pats[idx]:
        return 0, []
    hits = []
    for rgx in pats[idx]:
        m = rgx.search(text)
        if m: hits.append(m.group(0))
    if not hits: return 0, []
    # íœ´ë¦¬ìŠ¤í‹± ìŠ¤ì½”ì–´ë§
    strong = any(re.search(r"100\s*%|ë¶€ì‘ìš©\s*ì—†", h, re.I) for h in hits)
    if idx == 1 and strong: return 5, hits
    if idx in [3,5,7,14] and len(hits) >= 2: return 5, hits
    # ê¸°ë³¸: 1ê°œ ë°œê²¬=2, 2ê°œ ì´ìƒ=3 (í•„ìš”ì‹œ ì„¸ë¶„í™”)
    return (2 if len(hits) == 1 else 3), hits

def rule_score_all(title: str, content: str, pats: Dict[int, List[re.Pattern]]) -> Dict[str, Dict[str, Any]]:
    text = f"{title}\n\n{content}"
    results: Dict[str, Dict[str, Any]] = {}
    for i in range(1, 16):
        s, hits = rule_score_item(i, text, pats)
        results[str(i)] = {"score": s, "hits": hits}
    return results

# ===== ê°€ì¤‘ ì´ì  =====
def parse_report_weights(md_path: Path) -> Dict[str, float]:
    # ê°„ë‹¨ íŒŒì„œ: 3.1 í…Œì´ë¸” ë¼ì¸ì—ì„œ ìˆ«ì ì¶”ì¶œ (ì—†ìœ¼ë©´ DEFAULT ì‚¬ìš©)
    try:
        md = _read_text(md_path)
        lines = md.splitlines()
        weights = {}
        in_table = False
        for ln in lines:
            if "| ìˆœìœ„ |" in ln and "ìš°ì„ ìˆœìœ„ ì ìˆ˜" in ln:
                in_table = True
                continue
            if in_table:
                if ln.strip().startswith("|------"):
                    continue
                if not ln.strip().startswith("|"):
                    break
                # | 1 | í—ˆìœ„Â·ê³¼ì¥ í‘œí˜„ | 8.6 | ...
                cells = [c.strip() for c in ln.strip().strip("|").split("|")]
                if len(cells) >= 3:
                    name = cells[1]; w_str = cells[2]
                    # name â†’ index ì—­ë§¤í•‘
                    idx = None
                    for k,v in CHECKLIST_NAMES.items():
                        if v in name:
                            idx = k; break
                    if idx:
                        try:
                            weights[str(idx)] = float(w_str)
                        except:
                            pass
        return weights if weights else DEFAULT_REPORT_WEIGHTS
    except Exception:
        return DEFAULT_REPORT_WEIGHTS

def weighted_total(final_scores: Dict[str,int], weights: Dict[str,float], evaluation_mode: str = "medical") -> float:
    if evaluation_mode == "seo":
        # SEO ëª¨ë“œ: ì‹¤ì œ ì ìˆ˜ì˜ í•©ê³„ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        return round(sum(final_scores.get(str(i), 0) for i in range(1, 10)), 1)
    else:
        # ì˜ë£Œë²• ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ (5ì  ë§Œì  ê¸°ì¤€)
        num = sum((final_scores.get(k,0)/5.0) * weights[k] for k in weights)
        den = sum(weights.values())
        return round((num/den)*100, 1) if den else 0.0

# ===== ì„ê³„ ë¹„êµ =====
def over_threshold(scores: Dict[str, int], criteria: Dict[str, Dict[str, int]], mode: str, evaluation_mode: str = "medical") -> List[int]:
    th = criteria.get(mode)
    if not th:
        raise ValueError(f"criteria ëª¨ë“œê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {mode}")
    violations = []
    for k, v in scores.items():
        key = str(k)
        try:
            idx = int(key)
        except ValueError:
            continue
        limit = th.get(key, 5)
        # SEOì™€ ì˜ë£Œë²• í‰ê°€ ê¸°ì¤€ ë‹¤ë¥´ê²Œ ì ìš©
        if evaluation_mode == "seo":
            if v < limit:  # SEO: ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ìœ„ë°˜
                violations.append(idx)
        else:
            if v > limit:  # ì˜ë£Œë²•: ì ìˆ˜ê°€ ë†’ìœ¼ë©´ ìœ„ë°˜
                violations.append(idx)
    return violations

def map_stage(violations: List[int]) -> str:
    if any(v in [1,2,3,5,7,9,12,14] for v in violations):
        return "content"
    if any(v in [6,10,11] for v in violations):
        return "both"
    if any(v in [4,8,15] for v in violations):
        return "content"
    return "content"

# ===== íŒ¨ì¹˜ ì ìš© =====
def apply_patches(title: str, content: str, patch_obj: Dict[str, Any]) -> Tuple[str, str]:
    new_title, new_content = title, content
    for u in patch_obj.get("patch_units", []):
        typ = u.get("type")
        scope = u.get("scope")
        before = u.get("before", "")
        after = u.get("after", "")
        if scope == "title":
            if typ == "replace":
                new_title = new_title.replace(before, after) if before else after
            elif typ == "insert":
                new_title = after
            elif typ == "delete" and before:
                new_title = new_title.replace(before, "")
        else:
            if typ == "replace" and before and before in new_content:
                new_content = new_content.replace(before, after)
            elif typ == "insert" and after:
                new_content += "\n\n" + after
            elif typ == "delete" and before:
                new_content = new_content.replace(before, "")
    return new_title, new_content

# ===== í”„ë¡¬í”„íŠ¸ ë¹Œë“œ =====
def build_eval_prompt(title: str, content: str, prompt_path: Path = EVAL_PROMPT_PATH, seo_metrics: Dict[int, int] = None) -> str:
    base = _read_text(prompt_path)

    # SEO ëª¨ë“œì—ì„œ ì‹¤ì œ ì¸¡ì •ê°’ê³¼ ì •ë‹µì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    if seo_metrics and "seo_evaluation_prompt" in str(prompt_path):
        # ê° í•­ëª©ë³„ ì •í™•í•œ ì ìˆ˜ ê³„ì‚°
        def get_correct_score(item_num, value):
            if item_num == 1:  # ì œëª© ê¸€ììˆ˜ (ê³µë°± í¬í•¨)
                if 26 <= value <= 48: return 12
                elif 49 <= value <= 69: return 9
                elif 15 <= value <= 25: return 6
                else: return 3
            elif item_num == 2:  # ì œëª© ê¸€ììˆ˜ (ê³µë°± ì œì™¸)
                if 15 <= value <= 30: return 12
                elif 31 <= value <= 56: return 9
                elif 10 <= value <= 14: return 6
                else: return 3
            elif item_num == 3:  # ë³¸ë¬¸ ê¸€ììˆ˜ (ê³µë°± í¬í•¨)
                if 1233 <= value <= 2628: return 15
                elif 2629 <= value <= 4113: return 12
                elif 612 <= value <= 1232: return 9
                else: return 5
            elif item_num == 4:  # ë³¸ë¬¸ ê¸€ììˆ˜ (ê³µë°± ì œì™¸)
                if 936 <= value <= 1997: return 15
                elif 1998 <= value <= 3400: return 12
                elif 512 <= value <= 935: return 9
                else: return 5
            elif item_num == 5:  # ì´ í˜•íƒœì†Œ ê°œìˆ˜
                if 249 <= value <= 482: return 10
                elif 483 <= value <= 672: return 8
                elif 183 <= value <= 248: return 6
                else: return 3
            elif item_num == 6:  # ì´ ìŒì ˆ ê°œìˆ˜
                if 298 <= value <= 632: return 10
                elif 633 <= value <= 892: return 8
                elif 184 <= value <= 297: return 6
                else: return 3
            elif item_num == 7:  # ì´ ë‹¨ì–´ ê°œìˆ˜
                if 82 <= value <= 193: return 10
                elif 194 <= value <= 284: return 8
                elif 54 <= value <= 81: return 6
                else: return 3
            elif item_num == 8:  # ì–´ë·°ì§• ë‹¨ì–´ ê°œìˆ˜
                if 0 <= value <= 7: return 8
                elif 8 <= value <= 14: return 6
                elif 15 <= value <= 21: return 4
                else: return 2
            elif item_num == 9:  # ë³¸ë¬¸ ì´ë¯¸ì§€
                if 3 <= value <= 11: return 8
                elif 4 <= value <= 11: return 6
                elif 4 <= value <= 11: return 4
                else: return 2
            return 0

        metrics_text = f"""

ì‹¤ì œ ì¸¡ì •ê°’ê³¼ ì •ë‹µ:
1. ì œëª© ê¸€ììˆ˜ (ê³µë°± í¬í•¨): {seo_metrics.get(1, 0)}ê¸€ì â†’ {get_correct_score(1, seo_metrics.get(1, 0))}ì 
2. ì œëª© ê¸€ììˆ˜ (ê³µë°± ì œì™¸): {seo_metrics.get(2, 0)}ê¸€ì â†’ {get_correct_score(2, seo_metrics.get(2, 0))}ì   
3. ë³¸ë¬¸ ê¸€ììˆ˜ (ê³µë°± í¬í•¨): {seo_metrics.get(3, 0)}ê¸€ì â†’ {get_correct_score(3, seo_metrics.get(3, 0))}ì 
4. ë³¸ë¬¸ ê¸€ììˆ˜ (ê³µë°± ì œì™¸): {seo_metrics.get(4, 0)}ê¸€ì â†’ {get_correct_score(4, seo_metrics.get(4, 0))}ì 
5. ì´ í˜•íƒœì†Œ ê°œìˆ˜: {seo_metrics.get(5, 0)}ê°œ â†’ {get_correct_score(5, seo_metrics.get(5, 0))}ì 
6. ì´ ìŒì ˆ ê°œìˆ˜: {seo_metrics.get(6, 0)}ê°œ â†’ {get_correct_score(6, seo_metrics.get(6, 0))}ì 
7. ì´ ë‹¨ì–´ ê°œìˆ˜: {seo_metrics.get(7, 0)}ê°œ â†’ {get_correct_score(7, seo_metrics.get(7, 0))}ì 
8. ì–´ë·°ì§• ë‹¨ì–´ ê°œìˆ˜: {seo_metrics.get(8, 0)}ê°œ â†’ {get_correct_score(8, seo_metrics.get(8, 0))}ì 
9. ë³¸ë¬¸ ì´ë¯¸ì§€: {seo_metrics.get(9, 0)}ê°œ â†’ {get_correct_score(9, seo_metrics.get(9, 0))}ì 

ìœ„ì˜ ì •ë‹µ ì ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”! ë‹¤ë¥¸ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì§€ ë§ˆì„¸ìš”!"""
        base = base + metrics_text

    enforce = "\n\në°˜ë“œì‹œ ìœ„ì˜ ì¶œë ¥ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ ì“°ì§€ ë§ˆì‹­ì‹œì˜¤."
    return base.replace("[ì—¬ê¸°ì— ì œëª© ì…ë ¥]", title).replace("[ì—¬ê¸°ì— ë³¸ë¬¸ ì…ë ¥]", content) + enforce

def build_regen_prompt(title: str, content: str, criteria_mode: str,
                       violations: List[int], hints: List[str]) -> str:
    base = _read_text(REGEN_PROMPT_PATH)
    vnames = [f"{CHECKLIST_NAMES[i]}({i})" for i in violations]
    violations_json = json.dumps(vnames, ensure_ascii=False)
    hints_json = json.dumps(hints or [], ensure_ascii=False)
    prompt = (base
              .replace("{title}", title)
              .replace("{content}", content)
              .replace("{criteria}", criteria_mode)
              .replace("{violations}", violations_json)
              .replace("{hints}", hints_json))
    return prompt

# ===== ì¬ìƒì„± ì í•©ë„(0~100) =====
RISK_KEYWORDS = {
    "ë¶€ì‘ìš©": [r"ë¶€ì‘ìš©", r"ì£¼ì˜ì‚¬í•­", r"ê°œì¸ì°¨", r"í•©ë³‘ì¦"],
    "ê°€ê²©ê³ ì§€": [r"ê°€ê²©", r"ë¹„ìš©", r"ì¶”ê°€\s*ë¹„ìš©", r"ë¶€ê°€ì„¸"],
    "ê·¼ê±°ì œì‹œ": [r"ì—°êµ¬|ì„ìƒ|ë°ì´í„°|ê·¼ê±°|ê°€ì´ë“œë¼ì¸"],
    "ìœ ì¸ì‚­ì œ": [r"ë¦¬ë·°\s*ì´ë²¤íŠ¸|ì¶”ì²¨|ì‚¬ì€í’ˆ|ê¸°í”„í‹°ì½˜|ëŒ€ê°€"],
    "ê³¼ì¥ì™„í™”": [r"100\s*%|ìµœê³ |ìœ ì¼|ì™„ì „\s*ë¬´í†µ|ë¶€ì‘ìš©\s*ì—†"],
}

def _presence_rate(text: str, patterns: List[str]) -> float:
    if not patterns: return 0.0
    hits = sum(1 for p in patterns if re.search(p, text, re.I))
    return hits / len(patterns)

def regen_fit_score(before_over: List[int], after_over: List[int],
                    before_text: str, after_text: str,
                    tips: List[str]) -> Dict[str, Any]:
    # 1) ìœ„ë°˜í•´ì†Œìœ¨
    b = len(before_over); a = len(after_over)
    risk_reduction = (b - a) / b if b else 1.0

    # 2) ê¶Œê³  ë°˜ì˜ìœ¨
    adherence_checks = []
    for t in tips:
        t = str(t)
        key = None
        if any(k in t for k in ["ë¶€ì‘ìš©","ì£¼ì˜","ê°œì¸ì°¨"]): key = "ë¶€ì‘ìš©"
        elif any(k in t for k in ["ê°€ê²©","ë¹„ìš©","ë¶€ê°€ì„¸"]): key = "ê°€ê²©ê³ ì§€"
        elif any(k in t for k in ["ì—°êµ¬","ì„ìƒ","ë°ì´í„°","ê·¼ê±°"]): key = "ê·¼ê±°ì œì‹œ"
        elif any(k in t for k in ["ë¦¬ë·°","ì´ë²¤íŠ¸","ì¶”ì²¨","ì‚¬ì€í’ˆ","ëŒ€ê°€","ê¸°í”„í‹°ì½˜"]): key = "ìœ ì¸ì‚­ì œ"
        elif any(k in t for k in ["100%","ìµœê³ ","ìœ ì¼","ì™„ì „","ë¬´í†µ","ê³¼ì¥","ì ˆëŒ€"]): key = "ê³¼ì¥ì™„í™”"

        if key:
            pats = RISK_KEYWORDS[key]
            if key in ["ë¶€ì‘ìš©","ê°€ê²©ê³ ì§€","ê·¼ê±°ì œì‹œ"]:
                adherence_checks.append(_presence_rate(after_text, pats))
            else:
                before_r = _presence_rate(before_text, pats)
                after_r  = _presence_rate(after_text, pats)
                adherence_checks.append(1.0 if after_r < before_r else 0.0)

    guideline_adherence = sum(adherence_checks)/len(adherence_checks) if adherence_checks else 0.0

    # 3) íë¦„ ì•ˆì •ì„±
    def stats(s: str):
        paras = [p for p in s.split("\n\n") if p.strip()]
        sents = re.split(r"[.!?]\s+|[.\n]\s+", s)
        chars = len(s)
        return {
            "paras": len(paras) or 1,
            "sents": len([x for x in sents if x.strip()]) or 1,
            "chars": chars or 1
        }
    sb = stats(before_text); sa = stats(after_text)

    def stable_ratio(a,b): return max(0.0, 1.0 - abs(a-b)/max(a,1))
    flow = 0.5*stable_ratio(sa["paras"], sb["paras"]) + 0.3*stable_ratio(sa["sents"], sb["sents"]) + 0.2*stable_ratio(sa["chars"], sb["chars"])
    flow = max(0.0, min(flow, 1.0))

    final = round((0.5*risk_reduction + 0.3*guideline_adherence + 0.2*flow)*100)
    return {
        "risk_reduction_rate": round(risk_reduction, 3),
        "guideline_adherence": round(guideline_adherence, 3),
        "flow_stability": round(flow, 3),
        "score_0_100": final
    }

# ===== ë©”ì¸ ë£¨í”„ =====
def run(criteria_mode: str = "í‘œì¤€",
        max_loops: int = 2,
        auto_yes: bool = False,
        log_dir: Union[str, None] = None,
        pattern: Union[str, None] = None,
        debug: bool = False,
        csv_path: Union[str, None] = None,
        report_path: Union[str, None] = None,
        evaluation_mode: str = "medical"):

    # ë¡œê·¸ ë””ë ‰í† ë¦¬
    log_dir_path = Path(log_dir) if log_dir else DEFAULT_LOG_DIR
    log_dir_path.mkdir(parents=True, exist_ok=True)

    # íƒìƒ‰ íŒ¨í„´ì„ TXT íŒŒì¼ë¡œ ë³€ê²½
    patterns = [p.strip() for p in (pattern.split(",") if pattern else []) if p.strip()]
    search_patterns = patterns or [
        "*_title_content_result.txt",
        "*title_content*.txt", 
        "*content*.txt",
        "*_content_result.txt"
    ]

    # 0) TXT íŒŒì¼ ë¡œë“œ
    content_path = _latest(log_dir_path, search_patterns)
    
    # TXT íŒŒì¼ ì½ê¸°
    txt_content = _read_text(content_path)
    
    # ì²« ì¤„ì„ ì œëª©ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ë¥¼ ë³¸ë¬¸ìœ¼ë¡œ ë¶„ë¦¬
    lines = txt_content.split('\n')
    if lines:
        title = lines[0].strip()
        content = '\n'.join(lines[2:]).strip() if len(lines) > 2 else ""  # ì²« ì¤„ ì œëª©, ë‘˜ì§¸ ì¤„ ê³µë°±, ì…‹ì§¸ ì¤„ë¶€í„° ë³¸ë¬¸
    else:
        title = ""
        content = ""
    
    print(f"DEBUG - TXTì—ì„œ ì¶”ì¶œëœ ì œëª©: '{title}'")
    print(f"DEBUG - TXTì—ì„œ ì¶”ì¶œëœ ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
    
    if not title:
        raise ValueError(f"{content_path.name}ì—ì„œ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì½˜í…ì¸ ê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ì½˜í…ì¸  ì‚¬ìš©
    if not content:
        content = "ì œëª© í‰ê°€ìš© ë”ë¯¸ ì½˜í…ì¸ ì…ë‹ˆë‹¤."

    # SEO ëª¨ë“œì—ì„œ ì‹¤ì œ ì¸¡ì •ê°’ ê³„ì‚° (ì •ì œ ì ìš©)
    seo_metrics = {}
    if evaluation_mode == "seo":
        seo_metrics = calculate_seo_metrics(title, content)

    # 1) ê¸°ì¤€/CSV/ë¦¬í¬íŠ¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    if evaluation_mode == "seo":
        criteria = _read_json(SEO_CRITERIA_PATH)
        eval_prompt_path = SEO_PROMPT_PATH
    else:
        criteria = _read_json(CRITERIA_PATH)
        eval_prompt_path = EVAL_PROMPT_PATH
    if evaluation_mode == "medical":
        csv_file = Path(csv_path) if csv_path else _find_existing(DEFAULT_CSV_PATHS)
        rows = load_checklist_csv(csv_file)
        pats = compile_patterns(rows)
        report_file = Path(report_path) if report_path else _find_existing(DEFAULT_REPORT_PATHS)
        weights = parse_report_weights(report_file)
        # 2) ê·œì¹™ ê¸°ë°˜ ì‚¬ì „ ìŠ¤ì½”ì–´
        rule_all = rule_score_all(title, content, pats)
    else:
        # SEO ëª¨ë“œì—ì„œëŠ” ê·œì¹™ ê¸°ë°˜ í‰ê°€ ê±´ë„ˆë›°ê¸°
        rule_all = {}
        weights = {str(i): 1.0 for i in range(1, 10)}  # SEOëŠ” 9ê°œ í•­ëª©

    # 3) LLM í‰ê°€
    model = _setup_llm()
    if evaluation_mode == "seo":
        eval_prompt = build_eval_prompt(title, content, eval_prompt_path, seo_metrics)
    else:
        eval_prompt = build_eval_prompt(title, content, eval_prompt_path)
    result = _call_llm(model, eval_prompt)
    llm_scores: Dict[str, int] = result.get("í‰ê°€ê²°ê³¼", {}) or {}
    analysis: str = result.get("ìƒì„¸ë¶„ì„", "") or ""
    tips: List[str] = result.get("ê¶Œê³ ìˆ˜ì •", []) or []

    def fuse(rule_all: Dict[str, Dict[str,Any]], llm_scores: Dict[str,int]) -> Dict[str,int]:
        fused = {}
        max_items = 9 if evaluation_mode == "seo" else 15
        for i in range(1,max_items + 1):
            r = int(rule_all.get(str(i),{}).get("score",0))
            l = int(llm_scores.get(str(i),0))
            fused[str(i)] = max(r,l)
        return fused

    final_scores = fuse(rule_all, llm_scores)

    # 4) íŒì •/ê°€ì¤‘ ì´ì 
    violations_before = over_threshold(final_scores, criteria, criteria_mode, evaluation_mode)
    print(f"DEBUG - final_scores: {final_scores}")
    print(f"DEBUG - criteria[{criteria_mode}]: {criteria.get(criteria_mode)}")
    weighted_total_before = weighted_total(final_scores, weights, evaluation_mode)

    history: List[Dict[str, Any]] = []
    loop = 0
    patched_once = False
    title_before, content_before = title, content

    while True:
        loop += 1
        history.append({
            "loop": loop,
            "rule_scores": {k:v["score"] for k,v in rule_all.items()},
            "llm_scores": llm_scores,
            "final_scores": final_scores,
            "violations": violations_before,
            "analysis": analysis,
            "tips": tips
        })

        if not violations_before or loop >= max_loops:
            # ìµœì¢… ì‚°ì¶œ JSON
            out = {
                "input": {
                    "source_log": content_path.name,
                    "title": title,
                    "content": content,
                    "content_len": len(content)
                },
                "modes": {"criteria": criteria_mode},
                "scores": {
                    "by_item": {
                        str(i): {
                            "name": (SEO_CHECKLIST_NAMES[i] if evaluation_mode == "seo" else CHECKLIST_NAMES[i]),
                            "rule_score": int(rule_all.get(str(i),{}).get("score",0)),
                            "llm_score": int(llm_scores.get(str(i),0)),
                            "final_score": int(final_scores.get(str(i),0)),
                            "threshold": criteria[criteria_mode].get(str(i),5),
                            "passed": int(final_scores.get(str(i),0)) <= criteria[criteria_mode].get(str(i),5),
                            "evidence": {
                                "regex_hits": rule_all.get(str(i),{}).get("hits",[]),
                            },
                            **({"actual_value": seo_metrics.get(i, 0)} if evaluation_mode == "seo" else {})
                        } for i in range(1, 10 if evaluation_mode == "seo" else 16)
                    },
                    "weighted_total": weighted_total_before,
                    "llm_total_raw": sum(int(llm_scores.get(str(i),0)) for i in range(1, 10 if evaluation_mode == "seo" else 16)),
                    "rule_total_proxy": sum(int(rule_all.get(str(i),{}).get("score",0)) for i in range(1, 10 if evaluation_mode == "seo" else 16))
                },
                "violations": {
                    "over_threshold": violations_before,
                    "names": [(SEO_CHECKLIST_NAMES[i] if evaluation_mode == "seo" else CHECKLIST_NAMES[i]) for i in violations_before]
                },
                "regen_fit": {
                    "applied": patched_once
                },
                "notes": {
                    "recommendations": tips,
                    "report_weights": weights
                },
                "title": title,
                "content": content
            }

            # ì¬ìƒì„±ì´ ìˆì—ˆìœ¼ë©´ ì í•©ë„ ê³„ì‚°
            if patched_once:
                b_over = history[0]["violations"]
                a_over = violations_before
                before_text = f"{title_before}\n\n{content_before}"
                after_text  = f"{title}\n\n{content}"
                rf = regen_fit_score(b_over, a_over, before_text, after_text, tips)
                out["regen_fit"].update({
                    "before_over_threshold": len(b_over),
                    "after_over_threshold": len(a_over),
                    **rf
                })

            out_path = log_dir_path / f"{_nowstamp()}_evaluation.json"
            _write_json(out_path, out)
            
            # â­ UI checklist ë¡œê·¸ ìƒì„±
            generate_ui_checklist_logs(out, str(out_path))
            
            # â­ ìë™ DB ì—…ë°ì´íŠ¸
            auto_update_medicontent_posts(out, str(out_path))

            if patched_once:
                patched_path = log_dir_path / f"{_nowstamp()}_content.patched.json"
                _write_json(patched_path, {"title": title, "content": content})

            print(("âœ… ê¸°ì¤€ ì¶©ì¡±. " if not violations_before else "âš ï¸ ë°˜ë³µ ìƒí•œ ë„ë‹¬. ") +
                  f"ê²°ê³¼ ì €ì¥: {out_path.name}")
            return

        # í•„ìš” ì‹œ ì¬ìƒì„±
        if not auto_yes:
            yn = input(f"ê¸°ì¤€ ì´ˆê³¼ í•­ëª© {violations_before}ê°€ ìˆìŠµë‹ˆë‹¤. êµ­ì†Œ ìˆ˜ì • ì§„í–‰í• ê¹Œìš”? (Y/n): ").strip().lower()
            if yn and yn.startswith("n"):
                # ì¬ìƒì„± ê±°ë¶€ ì‹œì—ë„ í‰ê°€ ê²°ê³¼ ì €ì¥
                out = {
                    "input": {
                        "source_log": content_path.name,
                        "title": title,
                        "content": content,
                        "content_len": len(content)
                    },
                    "modes": {"criteria": criteria_mode},
                    "scores": {
                        "by_item": {
                            str(i): {
                                "name": (SEO_CHECKLIST_NAMES[i] if evaluation_mode == "seo" else CHECKLIST_NAMES[i]),
                                "rule_score": int(rule_all.get(str(i),{}).get("score",0)),
                                "llm_score": int(llm_scores.get(str(i),0)),
                                "final_score": int(final_scores.get(str(i),0)),
                                "threshold": criteria[criteria_mode].get(str(i),5),
                                "passed": int(final_scores.get(str(i),0)) <= criteria[criteria_mode].get(str(i),5),
                                "evidence": {
                                    "regex_hits": rule_all.get(str(i),{}).get("hits",[]),
                                },
                                **({"actual_value": seo_metrics.get(i, 0)} if evaluation_mode == "seo" else {})
                            } for i in range(1, 9 if evaluation_mode == "seo" else 16)
                        },
                        "weighted_total": weighted_total_before,
                        "llm_total_raw": sum(int(llm_scores.get(str(i),0)) for i in range(1, 9 if evaluation_mode == "seo" else 16)),
                        "rule_total_proxy": sum(int(rule_all.get(str(i),{}).get("score",0)) for i in range(1, 9 if evaluation_mode == "seo" else 16))
                    },
                    "violations": {
                        "over_threshold": violations_before,
                        "names": [(SEO_CHECKLIST_NAMES[i] if evaluation_mode == "seo" else CHECKLIST_NAMES[i]) for i in violations_before]
                    },
                    "regen_fit": {
                        "applied": False,  # ì¬ìƒì„± ê±°ë¶€í–ˆìœ¼ë¯€ë¡œ False
                        "user_declined": True  # ì‚¬ìš©ìê°€ ê±°ë¶€í–ˆë‹¤ëŠ” í‘œì‹œ
                    },
                    "notes": {
                        "recommendations": tips,
                        "report_weights": weights
                    },
                    "title": title,
                    "content": content
                }
        
                out_path = log_dir_path / f"{_nowstamp()}_evaluation.json"
                _write_json(out_path, out)
                print(f"âš ï¸ ì¬ìƒì„± ê±°ë¶€. ì›ë³¸ í‰ê°€ ê²°ê³¼ ì €ì¥: {out_path.name}")
                
                # â­ UI checklist ë¡œê·¸ ìƒì„±
                generate_ui_checklist_logs(out, str(out_path))
                
                # â­ ìë™ DB ì—…ë°ì´íŠ¸
                auto_update_medicontent_posts(out, str(out_path))
                
                return
                

        # ì¬ìƒì„± â†’ íŒ¨ì¹˜
        stage = map_stage(violations_before)
        regen_prompt = build_regen_prompt(title, content, criteria_mode, violations_before, tips)
        patch_obj = _call_llm(model, regen_prompt)
        title, content = apply_patches(title, content, patch_obj)
        patched_once = True

        # ì¬í‰ê°€ ì‚¬ì´í´: ê·œì¹™ + LLM ë‹¤ì‹œ
        if evaluation_mode == "medical":
            rule_all = rule_score_all(title, content, pats)
        else:
            rule_all = {}
        if evaluation_mode == "seo":
            eval_prompt = build_eval_prompt(title, content, eval_prompt_path, seo_metrics)
        else:
            eval_prompt = build_eval_prompt(title, content, eval_prompt_path)
        result = _call_llm(model, eval_prompt)
        llm_scores = result.get("í‰ê°€ê²°ê³¼", {}) or {}
        analysis = result.get("ìƒì„¸ë¶„ì„", "") or ""
        tips = result.get("ê¶Œê³ ìˆ˜ì •", []) or []
        max_items = 9 if evaluation_mode == "seo" else 15
        final_scores = {str(i): max(int(rule_all.get(str(i),{}).get("score",0)),
                                    int(llm_scores.get(str(i),0))) for i in range(1,max_items + 1)}
        violations_before = over_threshold(final_scores, criteria, criteria_mode)
        weighted_total_before = weighted_total(final_scores, weights, evaluation_mode)
        # ë‹¤ìŒ ë£¨í”„

# ===== CLI =====
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--criteria", default="í‘œì¤€", help="ì—„ê²© | í‘œì¤€ | ìœ ì—°")
    parser.add_argument("--max_loops", type=int, default=2)
    parser.add_argument("--auto-yes", action="store_true")
    parser.add_argument("--log-dir", default=str(DEFAULT_LOG_DIR), help="ë¡œê·¸ ë””ë ‰í† ë¦¬(ê¸°ë³¸: test_logs/test)")
    parser.add_argument("--pattern", default="", help="íƒìƒ‰ íŒ¨í„´(ì‰¼í‘œë¡œ ì—¬ëŸ¬ ê°œ). ë¹„ìš°ë©´ ê¸°ë³¸ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©")
    parser.add_argument("--csv-path", default="", help="medical_ad_checklist.csv ê²½ë¡œ(ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ ê²½ë¡œ/ /mnt/data íƒìƒ‰)")
    parser.add_argument("--report-path", default="", help="medical-ad-report.md ê²½ë¡œ(ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ ê²½ë¡œ/ /mnt/data íƒìƒ‰)")
    parser.add_argument("--debug", action="store_true", help="ì¶”ì¶œ í›„ë³´/ê²½ë¡œ ë””ë²„ê·¸ ë¡œê·¸ ì €ì¥")
    parser.add_argument("--evaluation-mode", default="medical", choices=["medical", "seo"], help="í‰ê°€ ëª¨ë“œ (medical: ì˜ë£Œë²•, seo: SEO í’ˆì§ˆ)")
    args = parser.parse_args()

    run(criteria_mode=args.criteria,
        max_loops=args.max_loops,
        auto_yes=args.auto_yes,
        log_dir=args.log_dir,
        pattern=args.pattern,
        debug=args.debug,
        csv_path=(args.csv_path or None),
        report_path=(args.report_path or None),
        evaluation_mode=args.evaluation_mode)